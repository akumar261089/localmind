# ğŸ§  LocalMind

**LocalMind** is a local-first AI interface that runs large language models directly in your browser using **WebGPU**.  
It is designed to be **private**, **transparent**, and **user-controlled**, with no backend servers involved.

LocalMind combines:

- A lightweight single-page website (Home / About / Features)
- Dedicated application pages for **Chat** and **AI-assisted Web Browsing**

---

## âœ¨ Key Principles

- **Local-first** â€” models run on your device
- **No backend** â€” no prompts or data are sent to servers
- **Explicit context** â€” the AI only sees what you give it
- **Browser-native** â€” respects browser security rules
- **Simple architecture** â€” no unnecessary frameworks

---

## ğŸ§© Project Structure

```

localmind/
â”œâ”€â”€ index.html
â”œâ”€â”€ chat.html
â”œâ”€â”€ browse.html
â”œâ”€â”€ css/
â”‚   â””â”€â”€ localmind.css
â”œâ”€â”€ main.chat.js
â”œâ”€â”€ main.browse.js
â””â”€â”€ README.md


```

---

## ğŸŒ Pages Overview

### 1ï¸âƒ£ Home (`index.html`)

A lightweight single-page site that explains:

- What LocalMind is
- Its philosophy and limitations
- Links to launch the apps

No models are loaded here.

---

### 2ï¸âƒ£ Chat (`chat.html`)

A pure LLM chat interface:

- Load and switch local models
- Quick model buttons
- Editable system prompts
- Model parameters (temperature, tokens)
- Streaming responses via WebGPU

This page is focused only on conversational AI.

---

### 3ï¸âƒ£ Browse (`browse.html`)

An AI-assisted browsing interface:

- Load any website in a browser frame
- Explicitly send page content to the AI
- Chat with the model _about_ the page
- Same model controls as Chat mode

âš ï¸ Due to browser security:

- Pages are **not read automatically**
- Only user-approved content is added to AI context

---

## ğŸ”’ Privacy & Security

LocalMind:

- Does **not** collect data
- Does **not** send prompts to external servers
- Does **not** bypass browser security (CORS, SOP)
- Does **not** track users

All computation happens locally using WebGPU.

---

## ğŸš€ Getting Started (Local Development)

### 1ï¸âƒ£ Clone or download the project

```bash
git clone <repo-url>
cd localmind
```

### 2ï¸âƒ£ Start a local server

```bash
python -m http.server
```

### 3ï¸âƒ£ Open in browser

```
http://localhost:8000
```

> âš ï¸ A local server is required.
> Opening files directly (`file://`) will not work.

---

## ğŸ§  Supported Technology

- **WebGPU** (Chrome / Edge recommended)
- **@mlc-ai/web-llm**
- ES Modules
- Vanilla HTML, CSS, JavaScript

---

## âš ï¸ Known Limitations

- Exact token counts are estimated, not exact
- Some websites block fetching due to CORS
- Large pages are truncated for context safety
- Performance depends on device GPU

These are inherent browser and WebGPU constraints.

---

## ğŸ›£ï¸ Roadmap (Possible Next Steps)

- Reader-mode text extraction
- Context size indicators
- Replace vs append page context toggle
- Page summary button
- Shared core logic between Chat & Browse
- Chrome extension for full page access
- Agent workflows

---

## ğŸ§  Philosophy

LocalMind is an experiment in building AI tools that:

- Users can understand
- Developers can inspect
- Machines can run locally

No hype. No magic. Honest limitations.

---

## ğŸ“„ License

This project is currently provided as-is for experimentation and learning.
Choose a license (MIT / Apache-2.0) before public release.

---

## ğŸ™Œ Acknowledgements

- WebLLM by MLC-AI
- WebGPU community
- Open-source LLM ecosystem
