# ğŸ§  Browser-Based LLM Chat (Web-LLM + Streaming)

A **fully client-side**, **serverless**, **streaming LLM chat UI** that runs directly in the browser using **WebGPU** and **@mlc-ai/web-llm**.

No backend.  
No API keys.  
No servers.

Runs entirely on the userâ€™s machine.

---

## âœ¨ Features

- ğŸš€ **Runs 100% in the browser**
- âš¡ **Token-by-token streaming output**
- ğŸ§  **Editable system prompt (locked after chat starts)**
- ğŸ›ï¸ Adjustable model parameters (temperature, top-p, max tokens)
- ğŸ”˜ Quick model buttons + editable model list
- ğŸ” Clean chat reset
- âŒ¨ï¸ Proper keyboard handling (Enter to send)
- ğŸ’¾ Quick models persisted via `localStorage`
- ğŸŒ GitHub Pages compatible (HTTPS + static hosting)

---

## ğŸ–¥ï¸ Supported Browsers

This app **requires WebGPU**.

### âœ… Works on

- Chrome (latest)
- Edge (latest)
- Chromium-based browsers with WebGPU enabled

### âŒ Not supported

- Safari (partial WebGPU support)
- Firefox (WebGPU behind flags)
- Mobile browsers

> âš ï¸ If `navigator.gpu` is not available, models will not load.

---

## ğŸ“¦ Models

Models are downloaded **at runtime** via CDN and run locally in the browser.

Example models:

- `Phi-3-mini-4k-instruct-q4f16_1`
- `TinyLlama-1.1B-Chat-v1.0-q4f16_1`

ğŸ“Œ **First load may take time** (models can be hundreds of MB).  
Subsequent loads are cached by the browser.

---

## ğŸ—‚ï¸ Project Structure

```

.
â”œâ”€â”€ index.html    # UI layout & styles
â”œâ”€â”€ main.js       # Chat logic, streaming, state management
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```

---

## ğŸš€ Running Locally

You must use a local web server (ES modules + WebGPU require it).

### Option 1: Python

```bash
python -m http.server 8000
```

Then open:

```
http://localhost:8000
```

### Option 2: VS Code Live Server

- Install **Live Server**
- Right-click `index.html` â†’ **Open with Live Server**

---

## ğŸŒ Deploying to GitHub Pages

1. Push files to GitHub:

```bash
git add .
git commit -m "Web-LLM streaming chat app"
git push origin main
```

2. Go to:

```
Repository â†’ Settings â†’ Pages
```

3. Configure:

- Source: `Deploy from a branch`
- Branch: `main`
- Folder: `/ (root)`

4. Save and wait ~30â€“60 seconds

Your app will be live at:

```
https://<username>.github.io/<repo-name>/
```

---

## ğŸ§  How It Works (High Level)

- Uses **@mlc-ai/web-llm** via CDN
- Loads models into WebGPU
- Maintains strict message roles:
  - `system` â†’ persona
  - `user` â†’ input
  - `assistant` â†’ response

- Uses **streaming completions** for real-time output
- Entire inference happens on the client machine

---

## ğŸ§ª Known Limitations

- Large model downloads on first use
- No mobile support
- No server-side tools or memory
- Browser GPU memory limits apply

---

## ğŸ” Privacy

âœ” All prompts and responses stay in the browser
âœ” No data is sent to a server
âœ” No tracking, no analytics, no logging

---

## ğŸ› ï¸ Future Improvements (Optional)

- â¹ Stop / cancel generation button
- ğŸ“œ Markdown rendering (code blocks)
- ğŸ§  Context trimming / memory management
- ğŸ§© Tool / function calling
- ğŸŒ WebGPU support detection UI
- ğŸ’¾ Per-model saved system prompts

---

## ğŸ“„ License

MIT License
Use freely for learning, demos, or personal projects.
